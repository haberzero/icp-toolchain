import sys
import os

# æ­£ç¡®æ·»åŠ src_mainç›®å½•åˆ°sys.pathï¼Œä»¥ä¾¿èƒ½å¤Ÿå¯¼å…¥libsä¸­çš„æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from utils.ibc_analyzer.ibc_lexer import Lexer, IbcTokenType, IbcKeywords, LexerError

def test_empty_file():
    """æµ‹è¯•ç©ºæ–‡ä»¶"""
    print("æµ‹è¯• empty_file å‡½æ•°...")
    
    code = ""
    expected = [
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.EOF, 'EOF')
    ]
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        assert len(tokens) == len(expected), f"Tokenæ•°é‡ä¸åŒ¹é…: é¢„æœŸ {len(expected)}, å®é™… {len(tokens)}"
        
        for i, (actual_token, expected_token) in enumerate(zip(tokens, expected)):
            expected_type, expected_value = expected_token
            assert actual_token.type == expected_type and actual_token.value == expected_value, \
                f"Token {i} ä¸åŒ¹é…: é¢„æœŸ Token({expected_type}, '{expected_value}', _) å®é™… {actual_token}"
        
        print("  âœ“ æˆåŠŸå¤„ç†ç©ºæ–‡ä»¶")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_comments_only():
    """æµ‹è¯•åªæœ‰æ³¨é‡Šçš„æ–‡ä»¶"""
    print("æµ‹è¯• comments_only å‡½æ•°...")
    
    code = """// è¿™æ˜¯ä¸€ä¸ªæ³¨é‡Š
// è¿™æ˜¯å¦ä¸€ä¸ªæ³¨é‡Š
"""
    expected = [
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.EOF, 'EOF')
    ]
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        assert len(tokens) == len(expected), f"Tokenæ•°é‡ä¸åŒ¹é…: é¢„æœŸ {len(expected)}, å®é™… {len(tokens)}"
        
        for i, (actual_token, expected_token) in enumerate(zip(tokens, expected)):
            expected_type, expected_value = expected_token
            assert actual_token.type == expected_type and actual_token.value == expected_value, \
                f"Token {i} ä¸åŒ¹é…: é¢„æœŸ Token({expected_type}, '{expected_value}', _) å®é™… {actual_token}"
        
        print("  âœ“ æˆåŠŸå¤„ç†åªæœ‰æ³¨é‡Šçš„æ–‡ä»¶")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_module_declaration():
    """æµ‹è¯•æ¨¡å—å£°æ˜"""
    print("æµ‹è¯• module_declaration å‡½æ•°...")
    
    code = """module requests: Pythonç¬¬ä¸‰æ–¹HTTPè¯·æ±‚åº“
module threading: ç³»ç»Ÿçº¿ç¨‹åº“
module utils"""
    expected = [
        (IbcTokenType.KEYWORDS, IbcKeywords.MODULE.value),
        (IbcTokenType.IDENTIFIER, 'requests'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' Pythonç¬¬ä¸‰æ–¹HTTPè¯·æ±‚åº“'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.KEYWORDS, IbcKeywords.MODULE.value),
        (IbcTokenType.IDENTIFIER, 'threading'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' ç³»ç»Ÿçº¿ç¨‹åº“'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.KEYWORDS, IbcKeywords.MODULE.value),
        (IbcTokenType.IDENTIFIER, 'utils'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.EOF, 'EOF')
    ]
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        assert len(tokens) == len(expected), f"Tokenæ•°é‡ä¸åŒ¹é…: é¢„æœŸ {len(expected)}, å®é™… {len(tokens)}"
        
        for i, (actual_token, expected_token) in enumerate(zip(tokens, expected)):
            expected_type, expected_value = expected_token
            assert actual_token.type == expected_type and actual_token.value == expected_value, \
                f"Token {i} ä¸åŒ¹é…: é¢„æœŸ Token({expected_type}, '{expected_value}', _) å®é™… {actual_token}"
        
        print("  âœ“ æˆåŠŸå¤„ç†æ¨¡å—å£°æ˜")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_function_declaration():
    """æµ‹è¯•å‡½æ•°å£°æ˜"""
    print("æµ‹è¯• function_declaration å‡½æ•°...")
    
    code = """func è®¡ç®—è®¢å•æ€»ä»·(å•†å“åˆ—è¡¨: åŒ…å«ä»·æ ¼ä¿¡æ¯çš„å•†å“å¯¹è±¡æ•°ç»„, æŠ˜æ‰£ç‡: 0åˆ°1ä¹‹é—´çš„å°æ•°):
    åˆå§‹åŒ– æ€»ä»· = 0"""
    expected = [
        (IbcTokenType.KEYWORDS, IbcKeywords.FUNC.value),
        (IbcTokenType.IDENTIFIER, 'è®¡ç®—è®¢å•æ€»ä»·'),
        (IbcTokenType.LPAREN, '('),
        (IbcTokenType.IDENTIFIER, 'å•†å“åˆ—è¡¨'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' åŒ…å«ä»·æ ¼ä¿¡æ¯çš„å•†å“å¯¹è±¡æ•°ç»„'),
        (IbcTokenType.COMMA, ','),
        (IbcTokenType.IDENTIFIER, ' æŠ˜æ‰£ç‡'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' 0åˆ°1ä¹‹é—´çš„å°æ•°'),
        (IbcTokenType.RPAREN, ')'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.INDENT_LEVEL, '1'),
        (IbcTokenType.IDENTIFIER, 'åˆå§‹åŒ– æ€»ä»· = 0'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.EOF, 'EOF')
    ]
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        assert len(tokens) == len(expected), f"Tokenæ•°é‡ä¸åŒ¹é…: é¢„æœŸ {len(expected)}, å®é™… {len(tokens)}"
        
        for i, (actual_token, expected_token) in enumerate(zip(tokens, expected)):
            expected_type, expected_value = expected_token
            assert actual_token.type == expected_type and actual_token.value == expected_value, \
                f"Token {i} ä¸åŒ¹é…: é¢„æœŸ Token({expected_type}, '{expected_value}', _) å®é™… {actual_token}"
        
        print("  âœ“ æˆåŠŸå¤„ç†å‡½æ•°å£°æ˜")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_class_declaration():
    """æµ‹è¯•ç±»å£°æ˜"""
    print("æµ‹è¯• class_declaration å‡½æ•°...")
    
    code = """class UserManager(BaseManager: ä½¿ç”¨å…¬å…±åŸºç±»ç®¡ç†ç”Ÿå‘½å‘¨æœŸ):
    var users: ç”¨æˆ·æ•°æ®å­—å…¸"""
    expected = [
        (IbcTokenType.KEYWORDS, IbcKeywords.CLASS.value),
        (IbcTokenType.IDENTIFIER, 'UserManager'),
        (IbcTokenType.LPAREN, '('),
        (IbcTokenType.IDENTIFIER, 'BaseManager'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' ä½¿ç”¨å…¬å…±åŸºç±»ç®¡ç†ç”Ÿå‘½å‘¨æœŸ'),
        (IbcTokenType.RPAREN, ')'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.INDENT_LEVEL, '1'),
        (IbcTokenType.KEYWORDS, IbcKeywords.VAR.value),
        (IbcTokenType.IDENTIFIER, 'users'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' ç”¨æˆ·æ•°æ®å­—å…¸'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.EOF, 'EOF')
    ]
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        assert len(tokens) == len(expected), f"Tokenæ•°é‡ä¸åŒ¹é…: é¢„æœŸ {len(expected)}, å®é™… {len(tokens)}"
        
        for i, (actual_token, expected_token) in enumerate(zip(tokens, expected)):
            expected_type, expected_value = expected_token
            assert actual_token.type == expected_type and actual_token.value == expected_value, \
                f"Token {i} ä¸åŒ¹é…: é¢„æœŸ Token({expected_type}, '{expected_value}', _) å®é™… {actual_token}"
        
        print("  âœ“ æˆåŠŸå¤„ç†ç±»å£°æ˜")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_description_and_intent_comment():
    """æµ‹è¯•æè¿°å’Œæ„å›¾æ³¨é‡Š"""
    print("æµ‹è¯• description_and_intent_comment å‡½æ•°...")
    
    code = """description: å¤„ç†ç”¨æˆ·ç™»å½•è¯·æ±‚ï¼ŒéªŒè¯å‡­æ®å¹¶è¿”å›è®¤è¯ç»“æœ
@ çº¿ç¨‹å®‰å…¨è®¾è®¡ï¼Œæ‰€æœ‰å…¬å…±æ–¹æ³•éƒ½å†…ç½®é”æœºåˆ¶
class AuthService():"""
    expected = [
        (IbcTokenType.KEYWORDS, IbcKeywords.DESCRIPTION.value),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' å¤„ç†ç”¨æˆ·ç™»å½•è¯·æ±‚ï¼ŒéªŒè¯å‡­æ®å¹¶è¿”å›è®¤è¯ç»“æœ'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.INTENT_COMMENT, 'çº¿ç¨‹å®‰å…¨è®¾è®¡ï¼Œæ‰€æœ‰å…¬å…±æ–¹æ³•éƒ½å†…ç½®é”æœºåˆ¶'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.KEYWORDS, IbcKeywords.CLASS.value),
        (IbcTokenType.IDENTIFIER, 'AuthService'),
        (IbcTokenType.LPAREN, '('),
        (IbcTokenType.RPAREN, ')'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.EOF, 'EOF')
    ]
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        assert len(tokens) == len(expected), f"Tokenæ•°é‡ä¸åŒ¹é…: é¢„æœŸ {len(expected)}, å®é™… {len(tokens)}"
        
        for i, (actual_token, expected_token) in enumerate(zip(tokens, expected)):
            expected_type, expected_value = expected_token
            assert actual_token.type == expected_type and actual_token.value == expected_value, \
                f"Token {i} ä¸åŒ¹é…: é¢„æœŸ Token({expected_type}, '{expected_value}', _) å®é™… {actual_token}"
        
        print("  âœ“ æˆåŠŸå¤„ç†æè¿°å’Œæ„å›¾æ³¨é‡Š")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_variable_declaration():
    """æµ‹è¯•å˜é‡å£°æ˜"""
    print("æµ‹è¯• variable_declaration å‡½æ•°...")
    
    code = """var userCount: å½“å‰åœ¨çº¿ç”¨æˆ·æ•°é‡
func test():
    var localVar: å±€éƒ¨å˜é‡"""
    expected = [
        (IbcTokenType.KEYWORDS, IbcKeywords.VAR.value),
        (IbcTokenType.IDENTIFIER, 'userCount'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' å½“å‰åœ¨çº¿ç”¨æˆ·æ•°é‡'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.KEYWORDS, IbcKeywords.FUNC.value),
        (IbcTokenType.IDENTIFIER, 'test'),
        (IbcTokenType.LPAREN, '('),
        (IbcTokenType.RPAREN, ')'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.INDENT_LEVEL, '1'),
        (IbcTokenType.KEYWORDS, IbcKeywords.VAR.value),
        (IbcTokenType.IDENTIFIER, 'localVar'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' å±€éƒ¨å˜é‡'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.EOF, 'EOF')
    ]
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        assert len(tokens) == len(expected), f"Tokenæ•°é‡ä¸åŒ¹é…: é¢„æœŸ {len(expected)}, å®é™… {len(tokens)}"
        
        for i, (actual_token, expected_token) in enumerate(zip(tokens, expected)):
            expected_type, expected_value = expected_token
            assert actual_token.type == expected_type and actual_token.value == expected_value, \
                f"Token {i} ä¸åŒ¹é…: é¢„æœŸ Token({expected_type}, '{expected_value}', _) å®é™… {actual_token}"
        
        print("  âœ“ æˆåŠŸå¤„ç†å˜é‡å£°æ˜")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_symbol_reference():
    """æµ‹è¯•ç¬¦å·å¼•ç”¨"""
    print("æµ‹è¯• symbol_reference å‡½æ•°...")
    
    code = """func å‘é€è¯·æ±‚(è¯·æ±‚æ•°æ®):
    å½“ é‡è¯•è®¡æ•° < $maxRetries$:"""
    expected = [
        (IbcTokenType.KEYWORDS, IbcKeywords.FUNC.value),
        (IbcTokenType.IDENTIFIER, 'å‘é€è¯·æ±‚'),
        (IbcTokenType.LPAREN, '('),
        (IbcTokenType.IDENTIFIER, 'è¯·æ±‚æ•°æ®'),
        (IbcTokenType.RPAREN, ')'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.INDENT_LEVEL, '1'),
        (IbcTokenType.IDENTIFIER, 'å½“ é‡è¯•è®¡æ•° < '),
        (IbcTokenType.REF_IDENTIFIER, 'maxRetries'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.EOF, 'EOF')
    ]
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        assert len(tokens) == len(expected), f"Tokenæ•°é‡ä¸åŒ¹é…: é¢„æœŸ {len(expected)}, å®é™… {len(tokens)}"
        
        for i, (actual_token, expected_token) in enumerate(zip(tokens, expected)):
            expected_type, expected_value = expected_token
            assert actual_token.type == expected_type and actual_token.value == expected_value, \
                f"Token {i} ä¸åŒ¹é…: é¢„æœŸ Token({expected_type}, '{expected_value}', _) å®é™… {actual_token}"
        
        print("  âœ“ æˆåŠŸå¤„ç†ç¬¦å·å¼•ç”¨")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_multiple_symbol_references():
    """æµ‹è¯•å¤šä¸ªç¬¦å·å¼•ç”¨"""
    print("æµ‹è¯• multiple_symbol_references å‡½æ•°...")
    
    code = """func test():
    $httpClient.post$(è¯·æ±‚æ•°æ®)
    $è®°å½•é”™è¯¯$("é…ç½®åŠ è½½å¤±è´¥: " + å¼‚å¸¸ä¿¡æ¯)"""
    expected = [
        (IbcTokenType.KEYWORDS, IbcKeywords.FUNC.value),
        (IbcTokenType.IDENTIFIER, 'test'),
        (IbcTokenType.LPAREN, '('),
        (IbcTokenType.RPAREN, ')'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.INDENT_LEVEL, '1'),
        (IbcTokenType.REF_IDENTIFIER, 'httpClient.post'),
        (IbcTokenType.LPAREN, '('),
        (IbcTokenType.IDENTIFIER, 'è¯·æ±‚æ•°æ®'),
        (IbcTokenType.RPAREN, ')'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.INDENT_LEVEL, '1'),
        (IbcTokenType.REF_IDENTIFIER, 'è®°å½•é”™è¯¯'),
        (IbcTokenType.LPAREN, '('),
        (IbcTokenType.IDENTIFIER, '"é…ç½®åŠ è½½å¤±è´¥'),
        (IbcTokenType.COLON, ':'),
        (IbcTokenType.IDENTIFIER, ' " + å¼‚å¸¸ä¿¡æ¯'),
        (IbcTokenType.RPAREN, ')'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.NEWLINE, 'NEWLINE'),
        (IbcTokenType.EOF, 'EOF')
    ]
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        assert len(tokens) == len(expected), f"Tokenæ•°é‡ä¸åŒ¹é…: é¢„æœŸ {len(expected)}, å®é™… {len(tokens)}"
        
        for i, (actual_token, expected_token) in enumerate(zip(tokens, expected)):
            expected_type, expected_value = expected_token
            assert actual_token.type == expected_type and actual_token.value == expected_value, \
                f"Token {i} ä¸åŒ¹é…: é¢„æœŸ Token({expected_type}, '{expected_value}', _) å®é™… {actual_token}"
        
        print("  âœ“ æˆåŠŸå¤„ç†å¤šä¸ªç¬¦å·å¼•ç”¨")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_error_cases():
    """æµ‹è¯•é”™è¯¯æƒ…å†µ"""
    print("æµ‹è¯• error_cases å‡½æ•°...")
    
    # æµ‹è¯•1: ä¸æˆå¯¹çš„$ç¬¦å·
    print("  1. æµ‹è¯•ä¸æˆå¯¹çš„$ç¬¦å·:")
    code1 = """func test():
    var ref = $unclosed_ref"""
    try:
        lexer = Lexer(code1)
        tokens = lexer.tokenize()
        # åº”è¯¥è¿”å›ç©ºåˆ—è¡¨
        assert len(tokens) == 0, "é¢„æœŸè¿”å›ç©ºåˆ—è¡¨ä½†å®é™…æ²¡æœ‰"
        print("    âœ“ æˆåŠŸæ£€æµ‹åˆ°ä¸æˆå¯¹çš„$ç¬¦å·")
    except Exception as e:
        print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•2: Tabç¼©è¿›
    print("  2. æµ‹è¯•Tabç¼©è¿›:")
    code2 = """func test():
\tvar tab_indented"""
    try:
        lexer = Lexer(code2)
        tokens = lexer.tokenize()
        # åº”è¯¥è¿”å›ç©ºåˆ—è¡¨
        assert len(tokens) == 0, "é¢„æœŸè¿”å›ç©ºåˆ—è¡¨ä½†å®é™…æ²¡æœ‰"
        print("    âœ“ æˆåŠŸæ£€æµ‹åˆ°Tabç¼©è¿›")
    except Exception as e:
        print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•3: ç¼©è¿›ä¸æ˜¯4çš„å€æ•°
    print("  3. æµ‹è¯•ç¼©è¿›ä¸æ˜¯4çš„å€æ•°:")
    code3 = """func test():
 var invalid_indent"""
    try:
        lexer = Lexer(code3)
        tokens = lexer.tokenize()
        # åº”è¯¥è¿”å›ç©ºåˆ—è¡¨
        assert len(tokens) == 0, "é¢„æœŸè¿”å›ç©ºåˆ—è¡¨ä½†å®é™…æ²¡æœ‰"
        print("    âœ“ æˆåŠŸæ£€æµ‹åˆ°ç¼©è¿›ä¸æ˜¯4çš„å€æ•°")
    except Exception as e:
        print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•4: ç©ºçš„ç¬¦å·å¼•ç”¨
    print("  4. æµ‹è¯•ç©ºçš„ç¬¦å·å¼•ç”¨:")
    code4 = """func test():
    var ref = $$"""
    try:
        lexer = Lexer(code4)
        tokens = lexer.tokenize()
        # è¿™ç§æƒ…å†µåªæ˜¯è­¦å‘Šï¼Œä¸ä¼šè¿”å›ç©ºåˆ—è¡¨ï¼Œåº”è¯¥æœ‰token
        assert len(tokens) > 0, "é¢„æœŸè¿”å›tokenåˆ—è¡¨ä½†å®é™…ä¸ºç©º"
        print("    âœ“ æˆåŠŸå¤„ç†ç©ºçš„ç¬¦å·å¼•ç”¨")
    except Exception as e:
        print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯• Intent Behavior Code è¯æ³•åˆ†æå™¨...\n")
    
    try:
        test_results = []
        
        test_results.append(("ç©ºæ–‡ä»¶", test_empty_file()))
        print()
        
        test_results.append(("åªæœ‰æ³¨é‡Š", test_comments_only()))
        print()
        
        test_results.append(("æ¨¡å—å£°æ˜", test_module_declaration()))
        print()
        
        test_results.append(("å‡½æ•°å£°æ˜", test_function_declaration()))
        print()
        
        test_results.append(("ç±»å£°æ˜", test_class_declaration()))
        print()
        
        test_results.append(("æè¿°å’Œæ„å›¾æ³¨é‡Š", test_description_and_intent_comment()))
        print()
        
        test_results.append(("å˜é‡å£°æ˜", test_variable_declaration()))
        print()
        
        test_results.append(("ç¬¦å·å¼•ç”¨", test_symbol_reference()))
        print()
        
        test_results.append(("å¤šä¸ªç¬¦å·å¼•ç”¨", test_multiple_symbol_references()))
        print()
        
        test_results.append(("é”™è¯¯æƒ…å†µ", test_error_cases()))
        print()
        
        print("æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 40)
        
        passed = 0
        failed = 0
        
        for test_name, result in test_results:
            status = "âœ“ é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"{test_name:20} {status}")
            if result:
                passed += 1
            else:
                failed += 1
        
        print(f"\næ€»è®¡: {passed} é€šè¿‡, {failed} å¤±è´¥")
        
        if failed == 0:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        else:
            print(f"âš ï¸  æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥")
        
        return failed == 0
        
    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)